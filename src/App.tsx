import { useState } from "react";

const code = `
21)ჩაწერეთ პროცედურა სიის კუთვნილება?


member(X,[X|_]).
member(X,[_|Kudi]):-
member(X,Kudi).
მიზანში member(a,[a,b,c]). დააბრუნებს ჭეშმარიტია

22)ჩაწერეთ პროცედურა გავიგოთ ლისტის სიგრძე?

list_length([],0).
list_length([_|L],N):-
list_length(L, N1),
N is N1 + 1.
მიზანში list_length([a,b,c,d],N) დააბრუნებს 4ს.


23)ჩაწერეთ პროცედურა სიების გადაბმა (კონკატენაცია)?

konkatenacia([], L2, L2).
konkatenacia([X | L1], L2, [X | L3]) :-
konkatenacia(L1, L2, L3).
მიზანში konkatenacia([a,b,c],[d,f,g],L). დააბრუნებს L = [a, b, c, d, f, g] 

24)ჩაწერეთ პროცედურა მიმდინარე თარიღის და დროის აღება ლოკალური კომპიუტერიდან?

get_date_time(Key, Value) :-
get_time(Stamp), %ეს ჩაშენებული ფუნქციაა
stamp_date_time(Stamp, DateTime, local),
date_time_value(Key, DateTime, Value).
მიზანში
?- get_date_time(day, X). დააბრუნებს მიმდინარე დღეს
?- get_date_time(year, X). წელს
?- get_date_time(date, X). დააბრუნებს მთლიან თარიღს



25)ჩაწერეთ პროცედურა გამოითვალეთ დაბადების წელი და თარიღი?

calc_age(Asaki, DabadebisWeli) :-
get_date_time(year, X),
Asaki is X - DabadebisWeli.
მიზანში
? – calc_age(X, 1991). დააბრუნებს ასაკს.

birth_year(Asaki, DabadebisTarigi) :-
get_date_time(year, X),
DabadebisTarigi is X- Asaki.
მიზანში
? - birth_year(33, DabadebisTarigi). დააბრუნებს დაბადების წელს



26) ჩაწერეთ პროცედურა ბაზების გამოყენებით?

q(blob,blug).
q(blob,blag).
q(blob,blig).
q(blaf,blag).
q(dang,dong).
q(dang,blug).
q(flab,blob).
მიზანში ამ ბრძანებებით შესაძლებელია მონაცმების პოვნა და დალაგება მონაცემთა ბაზაში.
findall(X,q(blob,X),List).
findall(X,q(X,blug),List).
findall(X,q(X,Y),List).
bagof(X,q(X,Y),List).
setof(X,Y^q(X,Y),List).


27) არითმეტიკული ოპერაციების გამოყენება გამოთვლებში?

შეკრება
?- მიზანში X is 1+3. 
გამოკლება
?- მიზანში X is (5-3)-2.
გაყოფა
?- მიზანში ?- X is 3/2,
Y is 3 div 2. პასუხი იქნება X = 1.5 Y = 1.
არითმეტიკული ოპერაციები
მაგალითად:
?- 277 * 37 > 10000. პასუხი იქნება yes;

?- 1 + 2 =:= 2+1. yes 
?- 1 + 2 = 2 + 1. No
?- 1 + A = B + 2. 
A = 2
B = 1



28) ჩაწერეთ პროცედურა სიაში ელემენტის დამატება (ფაქტი) და შეტანა (პროცედურა ამოშლის გამოყენებით) ?

damateba(X, L, [X | L]).
მიზანში damateba(a,[b,c,d],L). დააბრუნებს L = [a, b, c, d].


29) ჩაწერეთ პროცედურა სიიდან ელემენტის ამოშლა?

amoshla(X, [X | L], L).  %პირველ შემთხვევაში ამოშლის ელემენტს სიის თავიდან
amoshla(X, [Y | L], [Y | L1]):-  %ამოშლის ელემენტს სიის კუდიდან
amoshla(X, L, L1).
მიზანში amoshla(a,[a,b,c,d],L) დააბრუნებს L = [b, c, d]


30) ჩაწერეთ პროცედურა მოცემული ელემენტების კომბინაცია სიაში?

konkatenacia([], L2, L2).
konkatenacia([X | L1], L2, [X | L3]) :-
konkatenacia(L1, L2, L3).
მიზანში konkatenacia([a,b,c],[d,f,g],L). დააბრუნებს L = [a, b, c, d, f, g]


31) ჩაწერეთ პროცედურა ქვესია?

konk([], L, L).
konk([X | L1], L2, [X | L3]) :-
konk(L1, L2, L3).

qvesia (S, L) :-
konk(L1, L2, L),
konk(S, L3, L2).
მიზანში  ?- qvesia (S, [a, b, c]).


32) ჩაწერეთ პროცედურა ელემენტის გადანაცვლება სიაში?

% ელემენტის წაშლა სიიდან
amoshla(X, [X | Kudi], Kudi).
amoshla(X, [Y | Kudi], [Y | Kudi1]) :-
amoshla(X, Kudi, Kudi1).

% ელემენტის დამატება სიაში
shetana(X, L, L1) :-
amoshla(X, L1, L).
% სიის გარდაქმნა
gadanacvleba([], []).
gadanacvleba([X | L], P) :-
gadanacvleba(L, L1),
shetana(X, L1, P).

% სიის გარდაქმნის ალტერნატიული ვერსია
gadanacvleba2([], []).
gadanacvleba2(L, [X | P]) :-
amoshla(X, L, L1),
gadanacvleba2(L1, P).  
?- მიზანში gadanacvleba([1, 2, 3], Result).


33) ჩაწერეთ პროცედურა სიგრძის გამოთვლა?

list_length([],0).
list_length([_|L],N):-
list_length(L, N1),
N is N1 + 1.
მიზანში list_length([a,b,c,d],N) დააბრუნებს 4ს.
`;

function App() {
    const [prologCode] = useState(code);
    const [prologData] = useState({
        "რამ განაპირობა პერსეპტრონის ნეირონის განვითარება და როგორია მისი მოქმედების პრინციპი??": "პერსეპტრონის განვითარება გამოწვეული იყო მეცნიერებისა და ინჟინერიის მცდელობით, ხელოვნურად გაემეორებინათ ადამიანის ტვინის ნეირონების მუშაობის პრინციპი და შეექმნათ მოდელი, რომელსაც შეეძლებოდა დამოუკიდებლად სწავლა და გადაწყვეტილებების მიღება. პერსეპტრონის მუშაობის პრინციპი პერსეპტრონი არის მარტივი ხელოვნური ნეირონი, რომელიც მუშაობს შემდეგი პრინციპით: ნეირონს მიეწოდება შესატანი მონაცემები x_1, x_2, …, x_n, რომლებიც შეიძლება წარმოდგენილი იყოს როგორც რიცხვები ან სხვა სახის სიგნალები. თითოეულ შესავალ x_i-ს აქვს წონა w_i, რომელიც განსაზღვრავს ამ შესატანის გავლენას საბოლოო შედეგზე. ნეირონის შიგნით თითოეული შესატანი მონაცემი მრავლდება შესაბამის წონაზე და ჯამდება: ამოსავალი სიგნალის გამოსაწვევად ნეირონი იყენებს ზღვრულ მნიშვნელობას. თუ აწონილი ჯამი S მეტი ან ტოლია გარკვეული ბარიერის (თვითნებურად განსაზღვრული რიცხვი), ნეირონი „აქტიურდება“ და აბრუნებს ერთიანს (1). წინააღმდეგ შემთხვევაში, იგი „ინერტულია“ და აბრუნებს ნულს (0)",
        "რას ნიშნავს პერსეტრონული წარმოდგენადობა და სწავლებადობა და განიხილეთ შესაბამისი პრაქტიკული მაგალითი??": "წარმოდგენადობის ცნება მიეკუთვნება პერსეპტრონის უნარს მოდელირება გაუკეთოს გარკვეულ ფუნქციას. სწავლებადობა კი მოითხოვს, ამ ფუნქციის რეალიზაციისათვის, ქსელის წონების შეწყობის (კორექტირების) სისტემატური პროცედურების არსებობას. მაგალითი: “დავუშვათ, რომ გვაქვს 0-დან 9-მდე ციფრებით დანომრილი რამდენიმე ბარათი. დავუშვათ აგრეთვე, რომ გვაქვს ჰიპოტეტური მანქანა, რომელსაც შეუძლია გაარჩიოს კენტ ციფრებიანი ბარათები ლუწ ციფრებიანებისგან და თავის პანელზე აანთებს ინდიკატორს კენტ ციფრიანი ბარათების წარდგენისას.”წარმოდგენადია თუ არა ასეთი მანქანა პერცეპტრონით? ანუ შეიძლება თუ არა დაგეგმარდეს პერსეპტრონი და შეწყობილი იქნეს მისი წონები (არა აქვს მნიშვნელობა რა მეთოდით) ისე, რომ მას ჰქონდეს ასეთი გაყოფის უნარი? თუ ეს ასეა, მაშინ ამბობენ, რომ პერსეპტრონს შეუძლია წარმოადგინოს სასურველი მანქანა.",
     "დაახასიათეთ ერთ ნეირონიანი, ერთ შრიანი და ორ შესავლიანი სისტემის მოქმედების პრინციპი თავისი ჭეშმარიტების ცხრილით??": "ერთშრიან პერსეპტრონს არ შეუძლია ისეთი მარტივი ფუნქციის 7 წარმოდგენა, როგორიცაა ლოგიკური ”გამომრიცხავი ან”. ეს ორი არგუმენტის ფუნქციაა, რომელთაგანაც თითოეული შეიძლება იყოს ნული ან ერთი. ფუნქცია უდრის ერთს, თუ ერთერთი არგუმენტი უდრის ერთს (მაგრამ არა ორივე). პრობლემის ილუსტრირება შეიძლება ერთშრიანი, ორშესავლიანი ერთნეირონიანი სისტემით.",
     "რა არის წრფივი გაუყოფადობა და გაყოფადობა და რა ნიშნებით ხასიათდება ასეთი ქსელები??": "არსებობს საკმაოდ დიდი კლასი ფუნქციებისა, რომლებიც არ რეალიზდება ერთშრიანი ქსელით. ასეთ ფუნქციებს წრფივად გაუყოფადებს უწოდებენ და ისინი გარკვეულ შეზღუდვებს ადებენ ერთშრიანი ქსელების შესაძლებლობებს. წრფივი გაყოფადობა ერთშრიან ქსელებს ზღუდავს კლასიფიკაციის ამოცანებით, რომლებშიც წერტილთა სიმრავლეები (რომლებიც შეესაბამება შემავალ მნიშვნელობებს) შეიძლება გაიყოს გეომეტრიულად. ქსელების მახასიათებლები: • წრფივად გაყოფადი ქსელები ხასიათდებიან იმით, რომ მათი შემავალი მნიშვნელობები (წერტილები) შეიძლება გაიყოს „გამყოფი“ გეომეტრიული ობიექტით (მაგ., წრფე, სიბრტყე, ან ჰიპერსიბრტყე). • წრფივად გაუყოფადი ქსელები მოითხოვენ უფრო რთულ სტრუქტურას (მაგ., მრავალშრიან ქსელებს) წერტილების სწორად გასაყოფად, რადგან ერთი წრფე არ არის საკმარისი ასეთი ფუნქციების მოდელირებისთვის.",
     "რა უპირატესობა აქვს დამატებითი შრის შემოღებას და რომელ ლოგიკურ ფუნქციას უკეთებს ის რეალიზებას??": "ერთშრიანი ნეირონული ქსელების წარმოდგენადობის სერიოზული შეზღუდვების მოხსნა შესაძლებელია დამატებითი შრის შემოტანით. მაგალითად ორშრიანი ქსელის მიღება შეიძლება ორი ერთშრიანი ქსელის კასკადური შეერთებით..მეორე შრის ნეირონი რეალიზაციას უკეთებს ლოგიკურ ფუნქციას ”და”.უნქციას.",
     "როგორ აისახება შემავალ შრეში ნეირონების დამატება ქსელის ფუნქციონირებაზე და რომელი ლოგიკური ფუნქციების რეალიზაციას უწყობს ის ხელს??": "შემავალ შრეში საკმაო რაოდენობის ნეირონების ჩართვით შეიძლება მივიღოთ ნებისმიერი სასურველი ფორმის ამოზნექილი მრავალკუთხედი. რადგანაც ისინი მიღებულია არეებზე ლოგიკური ოპერაცია ”და”-ს საშუალებით მოცემული წრფეებით, წარმოიქმნება მხოლოდ ამოზნექილი არეები მეორე შრის ნეირონი არ არის შეზღუდული ”და” ფუნქციით. შესაბამისი წონებისა და ზღუდის შერჩევით ის რეალიზაციას გაუკეთებს მრავალ სხვა ფუნქციას. მაგალითად, შეგვიძლია გავაკეთოთ ისე, რომ პირველი შრის ნებისმიერი ნეირონის ერთეულოვანმა გამოსავალმა, მეორე შრის ნეირონზე მოგვცეს ერთეულოვანი გამოსავალი, ანუ რეალიზაცია გავუკეთოთ ფუნქციას ”ან”-ს. არსებობს 16 ორობითი ფუნქცია ორი ცვლადით. მათგან შეიძლება მოვახდინოთ 14-ის რეალიზაცია შესაბამისი წონებისა და ზღუდის შერჩევით (ყველა გარდა ”გამომრიცხავი ან” და ”არა არა”).",
    "რა უპირატესობა აქვს ნეირონების და წონების დამატებას გამოთვლის სიზუსტეში??": "ნეირონების და წონების დამატების შემთხვევაში მრავალკუთხედების გვერდების რაოდენობა შეიძლება შეუზღუდავად გაიზარდოს. ეს გვაძლევს ნებისმიერი ფორმის არის აპროქსიმირების საშუალებას, ნებისმიერი სიზუსტით. ამასთან აუცილებელი არ არის მეორე შრის ყველა ნეირონის გამომავალი არე გადაიკვეთოს. შესაბამისად, შესაძლებელია გავაერთიანოთ სხვადასხვა არეები, ამოზნექილები და ჩაზნექილები და მივიღოთ გამოსავალზე ერთიანი, როცა შემავალი ვექტორი ეკუთვნის ერთ-ერთ მათგანს.",
    "აღწერეთ პერსეპტონის აღწერის ალგორითმში ინდიკატორის ჩართვის ან არ ჩართვის შემთხვევები??": "პერსეპტრონის სწავლების დროს მას მიეწოდება სხვადასხვა შემავალი მონაცემი, მაგალითად, დემონსტრაციული ბარათები, რომლებიც დაყოფილია კვადრატებად. თითოეული კვადრატიდან პერსეპტრონს მიეწოდება სიგნალი: • “1” თუ კვადრატში არის ხაზი. • “0” თუ კვადრატში ხაზი არ არის. პერსეპტრონის მიზანია სწორად განსაზღვროს, მოცემული შემავალის მიხედვით ინდიკატორი უნდა იყოს ჩართული თუ არა: • ინდიკატორი ჩართულია (“1”), როცა შესავალი სიმრავლე კენტ რიცხვს აჩვენებს. • ინდიკატორი გამორთულია (“0”), როცა შესავალი სიმრავლე ლუწ რიცხვს აჩვენებს.",
    "ჩამოაყალიბეთ პერსეპტონის სწავლების ალგორითმის ეტაპები(3) და აღწერეთ რა შემთხვევაში დაყოფს ქსელი ბარათებს??": " მივაწოდოთ შემავალი სახე და გამოვთვალოთ Y. 2 ა. თუ გამოსავალი სწორია გადავიდეთ პირველ ბიჯზე; ბ. თუ გამოსავალი არასწორია და უდრის ნულს, მაშინ თითოეული შემავალი სიდიდე დავუმატოთ მის შესაბამის წონას; ან გ. თუ გამოსავალი არასწორია და უდრის ერთს, მაშინ თითოეული შემავალი სიდიდე გამოვაკლოთ მის შესაბამის წონას. 3. გადავიდეთ პირველ ბიჯზე. თუ ციფრების სიმრავლე წრფივად გაყოფადია, მაშინ ქსელი ისწავლის ბარათების დაყოფას კენტებად და ლუწებად სასრული რაოდენობის ბიჯებით. ეს ნიშნავს, რომ კენტციფრებიანი ბარათებისთვის გამოსავალი ზღუდეზე მეტი იქნება, ხოლო ლუწციფრებიანებისთვის - ნაკლები. შევნიშნოთ, რომ ეს სწავლება გლობალურია, ანუ ქსელი სწავლობს ბარათების მთელ სიმრავლეზე. ამ დროს წარმოიშობა საკითხი თუ როგორ უნდა წარმოვუდგინოთ ქსელს ეს სიმრავლე, რომ სწავლების დრო იყოს მინიმალური",
    "რა არის პერსეპტონის სწავლების მაგალითის დელტა წესი, როგორ მივიღოთ ის და რით გამსხვავდება ჩვეულებრივი ალგორითმისგან??": "პერსეპტრონის სწავლების ალგორითმის განზოგადებას, რომელსაც დელტა-წესი ეწოდება, გადაჰყავს ეს მეთოდი უწყვეტ შესავალ და გამოსავალ სიგნალებზე. რომ გავიგოთ თუ როგორ იქნა მიღებული ეს წესი, პერსეპტრონის სწავლების ალგორითმის მეორე ბიჯი უნდა ჩამოვაყალიბოთ განზოგადებული სახით, δ სიდიდის დახმარებით, რომელიც უდრის სხვაობას მიზნობრივ გამოსავალს T-ს და რეალურ გამოსავალს Y-ს შორის δ = (T - Y). დელტა-წესი მოდიფიცირებას უკეთებს წონებს თითოეული პოლარობის მოთხოვნილი და რეალური გამოსავლების შესაბამისად როგორც უწყვეტი, ასევე ბინარული შესავლებისა და გამოსავლებისათვის. ამ თვისებამ მრავალ ახალ შესაძლებლობას გაუხსნა გზა ქსელებში",
    "მრავალშრიანი ქსელისათვის როგორ მიმდინარეობს სწავლება უკუ გავრცელების პროცედურის დახმარებით??": "გავრცელების პროცედურის დახმარებით. უკუგავრცელება - მრავალშრიანი ნეირონული ქსელების სწავლების სისტემატური მეთოდია. მას სოლიდური მათემატიკური მტკიცებულებები გააჩნია. მიუხედავად ზოგიერთი შეზღუდვებისა, უკუგავრცელების პროცედურამ ძალიან გააფართოვა იმ პრობლემების არეალი, სადაც შეიძლება გამოყენებული იქნეს ხელოვნური ნეირონული ქსელები და დაადასტურა თავისი სიძლიერე. მრავალშრიანი ნეირონული ქსელების სწავლების სისტემატური მეთოდია.ნეირონების პირველი (შესავლებთან შეერთებული) შრე მხოლოდ სიგნალების გამანაწილებელ წერტილებს წარმოადგენს და აქ შეკრება არ მიმდინარეობს. შემავალი სიგნალი უბრალოდ გადის მათში, მათსავე გამოსავალზე არსებულ წონებთან. ხოლო შემდეგი შრეების თითოეული ნეირონი იძლევა NET და OUT სიგნალებს, როგორც ნაჩვენები იყო ზევით.",
    "უკუგავრცელების ქსელის სწავლებისას რომელი ოპერაციების შესრულებაა აუცილებელი და რა შემთხვევაში ამბობენ რომ ქსელმა ისწავლა??": "შემსწავლელი სიმრავლიდან შევარჩიოთ მორიგი შემსწავლელი წყვილი; მივაწოდოთ შემავალი ვექტორი ქსელის შესავალზე. 2. გამოვთვალოთ ქსელის გამოსავალი. 3. გამოვთვალოთ სხვაობა ქსელის გამოსავალსა და მოთხოვნილ გამოსავალს (შემსწავლელი წყვილის მიზნობრივი ვექტორი) შორის. 4. მოვახდინოთ წონების კორექტირება ისე, რომ მოხდეს შეცდომის მინიმიზირება. 5. გავიმეოროთ ბიჯები 1-დან 4-მდე შემსწავლელი სიმრავლის თითოეული ვექტორისთვის მანამ, სანამ შეცდომა მთელ სიმრავლეზე არ მიაღწევს მისაღებ მნიშვნელობას. ოთხი ბიჯის საკმაო რაოდენობით განმეორების შემდეგ ქსელის რეალურ გამოსავლებსა და მიზნობრივ ვექტორს შორის სხვაობა უნდა შემცირდეს მისაღებ სიდიდემდე. ამ დროს ამბობენ, რომ ქსელმა ისწავლა.",
        "რას ნიშნავს სწავლებისა და ნორმალური რეჟიმი? დაახასიათეთ სამკუთხით კოხონენის შრე??": "სხვა ქსელების ანალოგიურად შემხვედრი გავრცელების ქსელს აქვს ფუნქციონირების ორი რეჟიმი: სწავლების რეჟიმი, რომლის დროსაც შესავალზე მიეწოდება შემავალი ვექტორი და წონები კორექტირდება, რომ მივიღოთ საჭირო გამომავალი ვექტორი, და ნორმალური რეჟიმი, როდესაც ქსელი შესავალზე იღებს X ვექტორს და გამოსავალზე გვაძლევს Y ვექტორს.თავის უმარტივეს ფორმაში კოხონენის შრე ფუნქციონირებს პრინციპით: „გამარჯვებული იღებს ყველაფერს“, ანუ მოცემული ვექტორისათვის კოხონენის ერთი და მხოლოდ ერთი ნეირონი იძლევა გამოსავალზე ლოგიკურ ერთიანს, ყველა დანარჩენი კი ნულს.",
        "დაახასიათეთ გროსმერნის შრე და მიუთითეთ რა მსგავსება და განსხვავებაა კოხონენის შრესთან??": "გროსბერგის შრის ფუნქციონირება კოხონენის შრის ფუნქციონირების მსგავსია. მისი NET გამოსავალი წარმოადგენს კოხონენის შრის k1,k2, ..., kn გამოსავლების აწონილ ჯამს. თუ კოხონენის შრე ისე ფუნქციონირებს, რომ მხოლოდ ერთი ნეირონის NET სიდიდე უდრის ერთს, ხოლო ყველა დანარჩენის ნულს, მაშინ K ვექტორის მხოლოდ ერთი ელემენტია ნულისაგან განსხვავებული და გამოთვლები ძალიან მარტივია. ფაქტიურად გროსბერგის თითოეული ნეირონი იძლევა იმ წონის მნიშვნელობას, რომელიც მას აკავშირებს კოხონენის ერთადერთ არანულოვან ნეირონთან.",
        "აღწერეთ ხელოვნურ ნეირონული ქსელის სწავლების მეთოდები მათი ძირითადი პროცედურებით??": "არსებობს სწავლების ორი მეთოდი: დეტერმინისტული და სტოხასტიკური. სწავლების დეტერმინისტული მეთოდები ნაბიჯ- ნაბიჯ ასრულებენ ქსელის წონების კორექციის პროცედურას , რომელიც დაფუძნებულია მათი მიმდინარე მნიშვნელობების, შემავალი სიდიდეების, ფაქტიური და სასურველი გამომავალი სიდიდეების გამოყენებაზე. დეტერმინისტული მიდგომის ძალიან კარგი მაგალითია პერსეპტრონის სწავლება სწავლების სტოხასტიკური მეთოდები ახორციელებენ წონების ფსევდოშემთხვევით ცვლილებებს და ამასთან ინახავენ იმ ცვლილებებს, რომელთაც მივყავართ შედეგების გაუმჯობესებისაკენ. ქსელის სწავლებისთვის შეიძლება გამოვიყენოთ შემდეგი პროცედურა: 1. შევარჩიოთ წონა შემთვევით და გავუკეთოთ კორექცია მცირე შემთხვევითი მნიშვნელობით. წარვუდგინოთ ქსელს შემავალი სიმრავლე და გამოვთვალოთ გამომავალი სიდიდეები. 2. შევადაროთ მიღებული გამომავალი სიდიდეები სასურველს და გამოვთვალოთ მათ შორის სხვაობა. საერთოდ მიღებულია ამ სხვაობის გამოთვლა შემსწავლელი წყვილის თითოეული ელემენტისათვის, მათი კვადრატში აყვანა და ამ კვადრატების ჯამის გამოთვლა. სწავლების მიზანია ამ სხვაობის მინიმიზაცია, რომელსაც ხშირად მიზნობრივ ფუნქციასაც უწოდებენ. 3. შევარჩიოთ წონა შემთვევით და გავუკეთოთ კორექცია მცირე შემთხვევითი 57 მნიშვნელობით. თუ კორექცია გვეხმარება (ამცირებს მიზნობრივ ფუნქციას), ის შევინახოთ, წინააღმდეგ შემთხვევაში დავუბრუნდეთ საწყის წონას. 4. გავიმეოროთ ბიჯები 1-დან 3-ის ჩათვლით, სანამ ქსელი არ ისწავლის საკმარისი სიზუსტით.",
        "განიხილეთ სწავლების ბოლცმანის მეთოდი, რით განსხვავდება სხვა მეთოდებისგან??": "ეს სტოხასტიკური მეთოდი უშუალოდ შეიძლება გამოვიყენოთ ნეირონული ქსელის სასწავლებლად. 1. განვსაზღვროთ T ცვლადი, რომელიც წარმოადგენს ხელოვნურ ტემპერატურას. მივანიჭოთ T-ს დიდი საწყისი მნიშვნელობა. 2. წარვუდგინოთ ქსელს შემავალი სიმრავლე და გამოვთვალოთ გამომავალი სიდიდეები და მიზნობრივი ფუნქცია. 3. მივცეთ წონას შემთხვევითი ცვლილება და თავიდან გამოვთვალოთ ქსელის გამოსავალი და მიზნობრივი ფუნქციის ცვლილება, წონის ცვლილების შესაბამისად. 4. თუ მიზნობრივი ფუნქცია შემცირდა (გაუმჯობესდა), შევინახოთ წონის ცვლილება. თუ წონის ცვლილება იწვევს მიზნობრივი ფუნქციის გაზრდას, მაშინ ამ ცვლილების შენახვის ალბათობა გამოითვლება ბოლცმანის განაწილების საშუალებით P(c) = exp(–c/kT) ბოლცმანის მეთოდით სწავლების დასასრულებლად იმეორებენ მე-3 და მე-4 ბიჯებს ქსელის თითოეული წონისათვის T ტემპერატურის თანდათანობითი შემცირებით, სანამ არ მიიღწევა მიზნობრივი ფუნქციის ყველაზე დაბალი დასაშვები მნიშვნელობა. ამ მეთოდით გაცივების სიჩქარე ძალიან მცირე უნდა იყოს და შესაბამისად ბოლცმანის მანქანით სწავლების დრო ძალიან დიდია, რაც ექსპერიმენტულადაც დადასტურდა.",
        "განიხილეთ სწავლების კოშის მეთოდი რით განსხვავდება იგი სხვა მეთოდებისაგან??": "კოშის განაწილებას გააჩნია უფრო გრძელი “კუდები“ და ამით ზრდის დიდი ბიჯების ალბათობას. მართლაც, კოშის განაწილებას გააჩნია უსასრულო (განუსაზღვრელი) დისპერსია. ასეთი მარტივი ცვლილების საშუალებით ტემპერატურის შემცირების სიჩქარე ხდება არა ლოგარითმული სიდიდის უკუპროპორციული, როგორც ეს იყო ბოლცმანის განაწილებისას, არამედ -წრფივის. ეს კი მკვეთრად ამცირებს სწავლების დროს",
        "რით განსხვავდება ჰიფილდის ქსელი აქამდე შესწავლილი სხვა ქსელებისაგან??": "წინა თავებში განხილულ ქსელებს არ გააჩნდათ უკუკავშირები, ანუ კავშირები,რომლებიც მიდიან ქსელის გამოსასვლელიდან ქსელის შესასვლელისკენ. უკუკავშირების უქონლობა ქსელის მდგრადობის უცილობელ გარანტიას იძლევა. ისინი ვერ შედიან ისეთ რეჟიმში, როცა გამოსავალი დაუსრულებლად “დაბოდიალობს“ მდგომარეობიდან მდგომარეობაში და პრაქტიკული გამოყენებისათვის უვარგისია. მაგრამ ეს ძალიან სასურველი თვისება მსხვერპლის გარეშე არ მიიღწევა. ქსელებს უკუკავშირების გარეშე გააჩნიათ ბევრად უფრო შეზღუდული შესაძლებლობები, ვიდრე ქსელებს უკუკავშირებით. რადგანაც ქსელებს უკუკავშირებით გააჩნიათ არხები, რომლებითაც სიგნალები გადაეცემა გამოსავალიდან შესავალზე, ამიტომ ქსელის გამოძახილი (გამომავალი სიგნალი) დინამიურია.",
        "რას ნიშნავს ქსელის მდგრადობა და რა არის მდგრადობისთვის საკმარისი და აუვილებელი პირობა??": "მდგრად ქსელებზე, ანუ ისეთებზე, რომლებიც საბოლოოდ გვაძლევენ მუდმივ გამოსავალს. მდგრადობის პრობლემას ჩიხში შეჰყავდა პირველი მკვლევარები. არავის შეეძლო ეწინასწარმეტყველა რომელი ქსელი იყო მდგრადი და რომელი-არა. მეტიც, პრობლემა იმდენად რთული ჩანდა, რომ ბევრი მკვლევარი პესიმისტურად განეწყო საბოლოო ამოხსნის მიმართ. საბედნიეროდ [2] ნაშრომში მიღებულ თეორემაში აღწერილია უკუკავშირებიანი ქსელების ქვესიმრავლე, რომელთა გამოსავლები ბოლოს და ბოლოს აღწევენ მდგრად მდგომარეობას.",
        "დაახასიათეთ ასოციატიური მეხსიერება, რომელი ქსელები შეისწავლიან მას და როგორ მივიღოთ ლოკალური მინიმუმები??": "ლოკალური მინიმუმები. უკუგავრცელების ალგორითმში ქსელის წონების კორექციისათვის გამოყენებულია გრადიენტული დაშვება, რომელიც მიისწრაფვის მინიმუმისკენ შეცდომის ზედაპირის ლოკალური დახრის შესაბამისად. ის კარგად მუშაობს ძლიერად დაჩეხილი არაამობურცული ზედაპირებისთვის, რომლებიც ხშირად გვხვდება პრაქტიკულ ამოცანებში. ზოგიერთ შემთხვევაში ლოკალური მინიმუმის მონახვა მისაღებია, ზოგიერთში-არა. იმის შემდეგაც კი როცა ქსელი ნასწავლია, შეუძლებელია თქმა მონახულია თუ არა გლობალური მინიმუმი უკუგავრცელების ალგორითმით. თუ ამოხსნა არადამაკმაყოფილებელია, იძულებული ვხდებით მივცეთ წონებს ახალი საწყისი შემთხვევითი მნიშვნელობები და თავიდან ვასწავლოთ ქსელს ყოველგვარი გარანტიის გარეშე, რომ ამ ცდაზე სწავლება წარმატებული იქნება ან სართოდ მოინახება გლობალური მინიმუმი.",
    });

    return (
        <>
            <div style={{ color: "#e0e0e0", fontFamily: '"Fira Code", monospace', padding: "10px" }}>
                {Object.entries(prologData).map(([question, answer]) => (
                    <div key={question} style={{ marginBottom: "20px", padding: "10px"}}>
                        <h3 style={{ color: "#ffcc00" }}>{question}</h3>
                        <p style={{ whiteSpace: "pre-wrap", lineHeight: "1.5" }}>{answer}</p>
                    </div>
                ))}
            </div>

            <pre style={{
                color: '#e0e0e0',
                padding: '20px', 
                borderRadius: '8px',
                fontFamily: '"Fira Code", monospace',
                overflowX: 'auto'
            }}>
                {prologCode}
            </pre>
        </>
    );
}

export default App;
